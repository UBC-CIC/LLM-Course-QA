{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"google/boolq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import Dict\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\" <s>[INST]\n",
    "      The following is a conversation between a human and a friendly AI.\n",
    "      The AI uses the information in the context to answer the question from the human.\n",
    "      It does not use any other information.\n",
    "      The answer should always be written EXACTLY as follows: 'True' or 'False', NEVER add additional text other than the words true or false.\n",
    "      This is the context:\n",
    "      {context}.\n",
    "      Instruction: Based on the above documents, provide a detailed answer for, {question}\n",
    "      Answer \"I don't know\"\n",
    "      if not present in the document. Never provide an answer that is not based on the context, even if it is a well known fact. Your answer should always be 'true' or 'false', never add additional text.\n",
    "      Solution:\n",
    "      [/INST]\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "          content_type = \"application/json\"\n",
    "          accepts = \"application/json\"\n",
    "\n",
    "          def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "              input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "              return input_str.encode(\"utf-8\")\n",
    "\n",
    "          def transform_output(self, output: bytes) -> str:\n",
    "              response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "              return response_json[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "parameters = {\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"temperature\": 1,\n",
    "            \"stop_sequences\": None,\n",
    "            }\n",
    "\n",
    "\n",
    "model = SagemakerEndpoint(\n",
    "            credentials_profile_name=\"###\", # input\n",
    "            endpoint_name=\"###\", # input\n",
    "            region_name=\"###\", # input\n",
    "            model_kwargs=parameters,\n",
    "            endpoint_kwargs={\"CustomAttributes\":\"###\", # input\n",
    "                             \"InferenceComponentName\": \"###\"}, # input\n",
    "            content_handler=content_handler,)\n",
    "\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer = []\n",
    "ground_truth = []\n",
    "for data in dataset['train']:\n",
    "  ground_truth.append(data['answer'])\n",
    "  response = chain.invoke({\"context\":data['passage'], \"question\": data['question']})\n",
    "  llm_answer.append(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_llm_response = []\n",
    "for response in llm_answer:\n",
    "    if \"True\" in response:\n",
    "        parsed_llm_response.append(\"True\")\n",
    "    elif \"False\" in response:\n",
    "        parsed_llm_response.append(\"False\")\n",
    "    else:\n",
    "        parsed_llm_response.append(\"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_str = []\n",
    "for truth in ground_truth:\n",
    "    if truth == True:\n",
    "        ground_truth_str.append(\"True\")\n",
    "    elif truth == False:\n",
    "        ground_truth_str.append(\"False\")\n",
    "    else:\n",
    "        ground_truth_str.append(\"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure accuracy between ground truth and llm response\n",
    "correct = 0\n",
    "for i in range(9427):\n",
    "    if parsed_llm_response[i] == ground_truth_str[i]:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'ground_truth': ground_truth_str, 'llm_response': parsed_llm_response})\n",
    "df.to_csv('llm_boolq.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
