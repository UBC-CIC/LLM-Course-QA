{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict\n",
    "from langchain_community.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"###\"  # input\n",
    "region_name = \"###\"  # input\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 10,\n",
    "}\n",
    "\n",
    "profile_name = \"###\"  # input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "file_path = 'recall_validation_dataset.jsonl'\n",
    "test_data = load_jsonl(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(model_name):\n",
    "    if model_name == 'mistral-inst':\n",
    "        template = \"\"\" <s>[INST]\n",
    "You are a helpful assistant that provides direct and concise answers based only on the provided information.\n",
    "Use the following information from the course information to answer the user's question. If the answer is not present in the provided information, your answer must only be 'I do not know the answer'.\n",
    "Do not refer to the fact that there are provided course documents in your answer, just directly answer the question.\n",
    "< -- COURSE INFORMATION -- >\n",
    "{context}\n",
    "< -- END COURSE INFORMATION -- >\n",
    "< -- QUESTION -- >\n",
    "{question}\n",
    "< -- END QUESTION -- >\n",
    "Solution:\n",
    "[/INST]\"\"\"\n",
    "    else: # default zephyr-7b-beta\n",
    "        template = \"\"\"<|system|> You are a helpful assistant that provides direct and concise answers based only on the provided information.</s>\n",
    "<|user|> Use the following information from the course documents to answer the user's question. If the answer is not present in the provided information, your answer must only be 'I do not know the answer'.\n",
    "< -- QUESTION -- >\n",
    "{question}\n",
    "< -- END QUESTION -- >\n",
    "< -- COURSE INFORMATION -- >\n",
    "{context}\n",
    "< -- END COURSE INFORMATION -- >\n",
    "</s>\n",
    "<|assistant|> \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=template, input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "llm_open_args = {\n",
    "    \"endpoint_name\": endpoint_name,\n",
    "    \"region_name\": region_name,\n",
    "    \"model_kwargs\": parameters,\n",
    "    \"content_handler\": content_handler\n",
    "}\n",
    "\n",
    "if profile_name != '':\n",
    "    llm_open_args[\"credentials_profile_name\"] = profile_name\n",
    "\n",
    "llm_open = SagemakerEndpoint(**llm_open_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt('zephyr-7b-beta')\n",
    "chain = prompt | llm_open\n",
    "\n",
    "workbook = openpyxl.Workbook()\n",
    "sheet = workbook.active\n",
    "\n",
    "# Add data to cells\n",
    "sheet['A1'] = 'context'\n",
    "sheet['B1'] = 'question'\n",
    "sheet['C1'] = 'answer'\n",
    "sheet['D1'] = 'response'\n",
    "sheet['E1'] = 'score'\n",
    "\n",
    "for i, data in enumerate(test_data):\n",
    "    if i%10 == 0:\n",
    "        print(f'Processing {i}th data')\n",
    "    context = data['context']\n",
    "    question = data['question']\n",
    "    answer = data['answer']\n",
    "    response = chain.invoke({\"context\":context, \"question\": question})\n",
    "    sheet[f'A{i+2}'] = context\n",
    "    sheet[f'B{i+2}'] = question\n",
    "    sheet[f'C{i+2}'] = answer\n",
    "    sheet[f'D{i+2}'] = response\n",
    "\n",
    "print('processed all data')\n",
    "workbook.save('example-zeph.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt('mistral-inst')\n",
    "chain = prompt | llm_open\n",
    "\n",
    "context = \"Midterm 1 is worth 15%. Midterm 2 is worth 20%. The final exam is worth 30%. The homework is worth 35%.\"\n",
    "question = \"Is the final next week?\"\n",
    "answer = \"No\"\n",
    "response = chain.invoke({\"context\":context, \"question\": question})\n",
    "print(f'Context: {context}')\n",
    "print(f'Question: {question}')\n",
    "print(f'Answer: {answer}')\n",
    "print(f'Response: {response}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
