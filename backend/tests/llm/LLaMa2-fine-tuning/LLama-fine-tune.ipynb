{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Specify the profile name and region\n",
    "profile_name = '###'  # input\n",
    "aws_region = '###'  # input\n",
    "\n",
    "# Create a session using the specified profile\n",
    "boto_session = boto3.Session(profile_name=profile_name, region_name=aws_region)\n",
    "\n",
    "# Pass the session to the SageMaker Session\n",
    "sagemaker_session = sagemaker.Session(\n",
    "    boto_session=boto_session, sagemaker_client=boto_session.client('sagemaker', region_name='us-west-2'))\n",
    "print(sagemaker_session.boto_region_name)\n",
    "\n",
    "# Get the execution role for SageMaker\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "\n",
    "# Get the default S3 bucket\n",
    "output_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# This will be useful for printing\n",
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "print(f\"{bold}AWS Region:{unbold} {aws_region}\")\n",
    "print(f\"{bold}AWS Role:{unbold} {aws_role}\")\n",
    "print(f\"{bold}Output Bucket:{unbold} {output_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "squad_dataset = load_dataset(\"squad_v2\", split=\"train\")\n",
    "\n",
    "questions, contexts, answers = [], [], []\n",
    "answer_num = 0\n",
    "non_answer_num = 0\n",
    "total = 0\n",
    "\n",
    "for example in squad_dataset:\n",
    "    total += 1\n",
    "    if total % 100 == 0:\n",
    "        questions.append(example['question'])\n",
    "        contexts.append(\"\")\n",
    "        answers.append(\"Based on the existing context, I don't know the answer to this question\")\n",
    "        non_answer_num += 1\n",
    "\n",
    "    else:\n",
    "        # Extract question and context\n",
    "        questions.append(example['question'])\n",
    "        contexts.append(example['context'])\n",
    "\n",
    "        # Check if the answer exits，if there is no answer for this question，set answer to \"No answer\"\n",
    "        if example['answers']['text']:\n",
    "            answers.append(example['answers']['text'][0])\n",
    "            answer_num += 1\n",
    "        else:\n",
    "            answers.append(\"Based on the existing context, I don't know the answer to this question\")\n",
    "            non_answer_num += 1\n",
    "\n",
    "    # Create form\n",
    "    transformed_data = {\n",
    "        \"question\": questions,\n",
    "        \"context\": contexts,\n",
    "        \"answer\": answers\n",
    "    }\n",
    "\n",
    "transformed_squad_dataset = Dataset.from_dict(transformed_data)\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = transformed_squad_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_and_test_dataset[\"train\"].to_json(\"train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "categories = ['Answerable', 'Non-answerable']\n",
    "counts = [answer_num, non_answer_num]\n",
    "\n",
    "# Plot the graph to compare Answerable and Non-answerable Questions in SQuAD 2.0\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(categories, counts, color=['blue', 'red'])\n",
    "plt.title('Comparison of Answerable and Non-answerable Questions in SQuAD 2.0')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Questions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "      \"prompt\": \"\"\"\n",
    "            The following is a conversation between a human and a friendly AI.\n",
    "            The AI uses the information in the context to answer the question from the human.\n",
    "            It does not use any other information.\n",
    "            Firstly distinguish whether question is asking true or false or asking specific recall question\n",
    "            If the question is asking true or false question, the answer should always contain: 'True' or 'False', One of these two words must present in the answers based on context.\n",
    "            If the quetsion is asking specific answer, answer the question based on context, and answer \"I don't know\" if not present in the context. Never provide an answer that is not based on the context, even if it is a well known fact.\n",
    "            This is the context:\n",
    "            {context}.\n",
    "            Instruction: Based on the above documents, provide a detailed answer for this question:\n",
    "            {question}.\n",
    "            Now according to provided context, please answer the question\n",
    "      \"\"\",\n",
    "      \"completion\": \"{answer}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# Get the default S3 bucket\n",
    "output_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "local_data_file = \"train.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}/squad_dataset_mistral\"\n",
    "\n",
    "# Use the SageMaker session for uploading\n",
    "S3Uploader.upload(local_path=local_data_file, desired_s3_uri=train_data_location, sagemaker_session=sagemaker_session)\n",
    "S3Uploader.upload(local_path=\"template.json\", desired_s3_uri=train_data_location, sagemaker_session=sagemaker_session)\n",
    "\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "boolq_dataset = load_dataset(\"google/boolq\")\n",
    "\n",
    "boolq_questions, boolq_contexts, boolq_answers = [], [], []\n",
    "boolq_true_num = 0\n",
    "boolq_false_num = 0\n",
    "\n",
    "for example in boolq_dataset[\"train\"]:\n",
    "    boolq_questions.append(example['question'])\n",
    "    boolq_contexts.append(example['passage'])\n",
    "    boolq_answers.append(str(example['answer']))\n",
    "\n",
    "    if example['answer'] == True:\n",
    "        boolq_true_num += 1\n",
    "    else:\n",
    "        boolq_false_num += 1\n",
    "\n",
    "    transformed_data = {\n",
    "        \"question\": boolq_questions,\n",
    "        \"context\": boolq_contexts,\n",
    "        \"answer\": boolq_answers\n",
    "    }\n",
    "\n",
    "transformed_boolq_dataset = Dataset.from_dict(transformed_data)\n",
    "print(len(transformed_boolq_dataset))\n",
    "print(transformed_boolq_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  # Set random seed to maintain result achiveable\n",
    "squad_random_indices = np.random.permutation(len(transformed_squad_dataset))[:2000]\n",
    "boolq_random_indices = np.random.permutation(len(transformed_boolq_dataset))[:2000]\n",
    "\n",
    "# Select data based on random index\n",
    "mini_squad_train_dataset = transformed_squad_dataset.select(squad_random_indices)\n",
    "mini_boolq_train_dataset = transformed_boolq_dataset.select(boolq_random_indices)\n",
    "\n",
    "# Save data to local file to be used for training.\n",
    "mini_squad_train_dataset.to_json(\"mini_train/squad_train.jsonl\")\n",
    "mini_boolq_train_dataset.to_json(\"mini_train/boolq_train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_answer_num = 0\n",
    "mini_non_answer_num = 0\n",
    "\n",
    "for example in mini_squad_train_dataset:\n",
    "\n",
    "    # Check if the answer exits，if there is no answer for this question，set answer to \"No answer\"\n",
    "    if example['answer'] == \"I don't know, I don't have the information or knowledge to provide an answer to your question. However, I'm here to assist you with any other inquiries you may have related to the course. Feel free to ask!\":\n",
    "        mini_non_answer_num += 1\n",
    "    else:\n",
    "        mini_answer_num += 1\n",
    "\n",
    "mini_true_num = 0\n",
    "mini_false_num = 0\n",
    "\n",
    "for example in mini_boolq_train_dataset:\n",
    "\n",
    "    if example['answer'] == \"True\":\n",
    "        mini_true_num += 1\n",
    "    else:\n",
    "        mini_false_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "categories = ['Recall question', 'Non-answerable', \"True Answer Question\", \"False Answer Question\"]\n",
    "counts = [mini_answer_num, mini_non_answer_num, mini_true_num, mini_false_num]\n",
    "\n",
    "# Plot the graph to compare Answerable and Non-answerable Questions in mini SQuAD 2.0\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(categories, counts, color=['blue', 'red', 'green', 'yellow'])\n",
    "plt.title('Comparison of Answerable and Non-answerable Questions in mini dataset')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Questions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# Get the default S3 bucket\n",
    "output_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "local_squad_data_file = \"mini_train/squad_train.jsonl\"\n",
    "local_boolq_data_file = \"mini_train/boolq_train.jsonl\"\n",
    "mini_train_data_location = f\"s3://{output_bucket}/mini_dataset_llama\"\n",
    "\n",
    "# Use the SageMaker session for uploading\n",
    "S3Uploader.upload(local_path=local_squad_data_file, desired_s3_uri=mini_train_data_location, sagemaker_session=sagemaker_session)\n",
    "S3Uploader.upload(local_path=local_boolq_data_file, desired_s3_uri=mini_train_data_location, sagemaker_session=sagemaker_session)\n",
    "S3Uploader.upload(local_path=\"template.json\", desired_s3_uri=mini_train_data_location, sagemaker_session=sagemaker_session)\n",
    "\n",
    "print(f\"Training data: {mini_train_data_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-7b-f\", \"3.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "my_hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=model_id, model_version=model_version, sagemaker_session=sagemaker_session\n",
    ")\n",
    "print(my_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hyperparameters[\"epoch\"] = \"1\"\n",
    "my_hyperparameters[\"per_device_train_batch_size\"] = \"1\"\n",
    "my_hyperparameters[\"int8_quantization\"] = 'True'\n",
    "my_hyperparameters[\"enable_fsdp\"] = 'False'\n",
    "my_hyperparameters[\"instruction_tuned\"] = \"True\"\n",
    "my_hyperparameters[\"chat_dataset\"] = \"False\"\n",
    "\n",
    "print(my_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters.validate(\n",
    "    model_id=model_id, model_version=model_version, hyperparameters=my_hyperparameters, sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "instruction_tuned_estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    region=\"us-west-2\",\n",
    "    hyperparameters=my_hyperparameters,\n",
    "    instance_type=\"###\", # input\n",
    "    role = \"###\", # input\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    environment={\"accept_eula\": \"true\"}\n",
    ")\n",
    "print(instruction_tuned_estimator.sagemaker_session.boto_region_name)\n",
    "print(mini_train_data_location)\n",
    "instruction_tuned_estimator.fit({\"train\": mini_train_data_location}, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "training_job_name = instruction_tuned_estimator.latest_training_job.job_name\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name, sagemaker_session=sagemaker_session).dataframe()\n",
    "df_eval_loss = df[df[\"metric_name\"].str.contains(\"eval-loss\")]\n",
    "df_train_loss = df[df[\"metric_name\"].str.contains(\"train-loss\")]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_eval_loss[\"timestamp\"], df_eval_loss[\"value\"], label='Eval Loss')\n",
    "plt.plot(df_train_loss[\"timestamp\"], df_train_loss[\"value\"], label='Train Loss')\n",
    "\n",
    "plt.title('Loss over Time')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_tuned_predictor = instruction_tuned_estimator.deploy(initial_instance_count=1, instance_type='ml.g5.2xlarge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "instruction_tuned_predictor.delete_model()\n",
    "instruction_tuned_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
